{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3275ace2",
   "metadata": {},
   "source": [
    "# Exercise 1 - Question 1 (Language Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eb6b6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3281737530.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Requirements: to be installed in the virtual environment\n",
    "\n",
    "# pip install torch\n",
    "# pip install transformers\n",
    "# pip install datasets\n",
    "# pip install numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "272c0a7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"ngram_lm\"></a>\n",
    "## Task A: Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "066c0427",
   "metadata": {},
   "source": [
    "\n",
    "Let's use an unsupervised dataset (raw corpus) to evaluate language models' perplexity. We use Huggingface's `datasets` library to download needed datasets.\n",
    " \n",
    "\n",
    "Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69afb9de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezradin/.conda/envs/pip_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset ptb_text_only (/home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n",
      "Loading cached split indices for dataset at /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6d72a72d5c72be3d.arrow and /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-3e2d0e2b1466183a.arrow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\")\n",
    "\n",
    "# splitting dataset in train/test (to be later used for language model evaluation)\n",
    "ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n",
    "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20589cd0",
   "metadata": {},
   "source": [
    "#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ceb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
      "\n",
      "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
      "\n",
      "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07207b13",
   "metadata": {},
   "source": [
    "During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n",
    "\n",
    "#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n",
    "Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1b7b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-9390bd991ee6f14d.arrow\n",
      "Loading cached processed dataset at /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-952de2249dda7f49.arrow\n"
     ]
    }
   ],
   "source": [
    "def add_stop_token(input_sample: dict):\n",
    "    '''\n",
    "    args:\n",
    "        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n",
    "    output:\n",
    "        modified_sample: modified dict adding <stop> at the end of each sentence.\n",
    "    '''\n",
    "    modified_sample ={}\n",
    "    modified_sample['sentence'] = input_sample['sentence'] + \" <stop>\"\n",
    "    \n",
    "    \n",
    "    return modified_sample\n",
    "    \n",
    "    \n",
    "ptb_train = ptb_train.map(add_stop_token)\n",
    "ptb_test = ptb_test.map(add_stop_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d338821",
   "metadata": {},
   "source": [
    "For the both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n",
    "\n",
    "Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0222f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-4dbad66706bdcdb0.arrow\n",
      "Loading cached processed dataset at /home/ezradin/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-e30e090ecece4408.arrow\n"
     ]
    }
   ],
   "source": [
    "ptb_train= ptb_train.filter(lambda dict: len(dict['sentence'])>3)\n",
    "ptb_test= ptb_test.filter(lambda dict: len(dict['sentence'])>3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2733f77b",
   "metadata": {},
   "source": [
    "#### What are the 10 most frequent tokens in this dataset? Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e93fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 40616,\n",
       "         '<unk>': 35918,\n",
       "         '<stop>': 33654,\n",
       "         'N': 25966,\n",
       "         'of': 19459,\n",
       "         'to': 18896,\n",
       "         'a': 16901,\n",
       "         'in': 14473,\n",
       "         'and': 14013,\n",
       "         \"'s\": 7850,\n",
       "         'for': 7108,\n",
       "         'that': 7100,\n",
       "         '$': 5957,\n",
       "         'is': 5924,\n",
       "         'it': 4870,\n",
       "         'said': 4840,\n",
       "         'on': 4494,\n",
       "         'at': 3946,\n",
       "         'by': 3939,\n",
       "         'as': 3860,\n",
       "         'from': 3818,\n",
       "         'with': 3678,\n",
       "         'million': 3655,\n",
       "         'mr.': 3468,\n",
       "         'was': 3245,\n",
       "         'be': 3122,\n",
       "         'are': 3111,\n",
       "         'its': 3089,\n",
       "         'he': 2898,\n",
       "         'but': 2841,\n",
       "         'has': 2818,\n",
       "         'an': 2787,\n",
       "         \"n't\": 2694,\n",
       "         'will': 2582,\n",
       "         'have': 2557,\n",
       "         'new': 2236,\n",
       "         'company': 2175,\n",
       "         'or': 2141,\n",
       "         'they': 2075,\n",
       "         'this': 1953,\n",
       "         'year': 1908,\n",
       "         'which': 1872,\n",
       "         'would': 1840,\n",
       "         'about': 1732,\n",
       "         'says': 1658,\n",
       "         'were': 1632,\n",
       "         'more': 1626,\n",
       "         'market': 1587,\n",
       "         'billion': 1505,\n",
       "         'his': 1490,\n",
       "         'had': 1466,\n",
       "         'their': 1444,\n",
       "         'one': 1413,\n",
       "         'up': 1408,\n",
       "         'u.s.': 1391,\n",
       "         'than': 1366,\n",
       "         'who': 1360,\n",
       "         'some': 1360,\n",
       "         'been': 1344,\n",
       "         'also': 1290,\n",
       "         'stock': 1280,\n",
       "         'other': 1267,\n",
       "         'share': 1203,\n",
       "         'not': 1159,\n",
       "         'we': 1094,\n",
       "         'if': 1029,\n",
       "         'when': 1018,\n",
       "         'corp.': 1015,\n",
       "         'i': 1005,\n",
       "         'years': 1002,\n",
       "         'all': 997,\n",
       "         'president': 996,\n",
       "         'last': 991,\n",
       "         'shares': 989,\n",
       "         'first': 954,\n",
       "         'trading': 952,\n",
       "         'two': 934,\n",
       "         'inc.': 923,\n",
       "         'because': 913,\n",
       "         'could': 896,\n",
       "         'sales': 889,\n",
       "         'after': 880,\n",
       "         'there': 873,\n",
       "         '&': 859,\n",
       "         'business': 854,\n",
       "         'out': 848,\n",
       "         'do': 808,\n",
       "         'only': 800,\n",
       "         'can': 795,\n",
       "         'such': 788,\n",
       "         'co.': 787,\n",
       "         'may': 780,\n",
       "         'york': 773,\n",
       "         'most': 766,\n",
       "         'over': 765,\n",
       "         'into': 763,\n",
       "         'group': 754,\n",
       "         'federal': 731,\n",
       "         'many': 727,\n",
       "         'government': 722,\n",
       "         'no': 703,\n",
       "         'now': 700,\n",
       "         'time': 681,\n",
       "         'bank': 672,\n",
       "         'companies': 667,\n",
       "         'so': 662,\n",
       "         'any': 660,\n",
       "         'people': 653,\n",
       "         'cents': 650,\n",
       "         'quarter': 636,\n",
       "         'you': 629,\n",
       "         'exchange': 627,\n",
       "         'what': 627,\n",
       "         'prices': 625,\n",
       "         'price': 613,\n",
       "         'even': 609,\n",
       "         'say': 605,\n",
       "         'while': 602,\n",
       "         'down': 600,\n",
       "         'rose': 593,\n",
       "         'much': 582,\n",
       "         'investors': 579,\n",
       "         'yesterday': 577,\n",
       "         'under': 569,\n",
       "         'big': 569,\n",
       "         'securities': 568,\n",
       "         'week': 568,\n",
       "         'them': 548,\n",
       "         'months': 546,\n",
       "         'bonds': 528,\n",
       "         'next': 526,\n",
       "         \"'\": 523,\n",
       "         'earnings': 522,\n",
       "         'major': 520,\n",
       "         'make': 520,\n",
       "         'interest': 520,\n",
       "         'earlier': 519,\n",
       "         'three': 519,\n",
       "         'american': 513,\n",
       "         'state': 513,\n",
       "         'financial': 512,\n",
       "         'net': 510,\n",
       "         'investment': 503,\n",
       "         'still': 502,\n",
       "         'chairman': 499,\n",
       "         'these': 497,\n",
       "         'since': 493,\n",
       "         'chief': 492,\n",
       "         'those': 489,\n",
       "         'did': 487,\n",
       "         'board': 484,\n",
       "         'before': 479,\n",
       "         'through': 479,\n",
       "         'program': 477,\n",
       "         'industry': 475,\n",
       "         'executive': 472,\n",
       "         'stocks': 471,\n",
       "         'made': 470,\n",
       "         'just': 466,\n",
       "         'officials': 464,\n",
       "         'rate': 462,\n",
       "         'national': 459,\n",
       "         'like': 446,\n",
       "         'money': 445,\n",
       "         'month': 441,\n",
       "         'analysts': 440,\n",
       "         'house': 431,\n",
       "         'rates': 427,\n",
       "         'she': 425,\n",
       "         'off': 422,\n",
       "         'profit': 421,\n",
       "         'expected': 420,\n",
       "         'against': 420,\n",
       "         'plan': 417,\n",
       "         'both': 417,\n",
       "         'does': 415,\n",
       "         'days': 410,\n",
       "         'unit': 408,\n",
       "         'between': 405,\n",
       "         'capital': 404,\n",
       "         'markets': 399,\n",
       "         'income': 399,\n",
       "         'buy': 393,\n",
       "         'recent': 392,\n",
       "         'get': 390,\n",
       "         'japanese': 387,\n",
       "         'sell': 387,\n",
       "         'issue': 384,\n",
       "         'firm': 383,\n",
       "         'well': 382,\n",
       "         'during': 381,\n",
       "         'ago': 378,\n",
       "         'average': 378,\n",
       "         'revenue': 377,\n",
       "         'own': 374,\n",
       "         'should': 373,\n",
       "         'back': 372,\n",
       "         'international': 372,\n",
       "         'general': 370,\n",
       "         'products': 370,\n",
       "         'her': 370,\n",
       "         'another': 368,\n",
       "         'funds': 366,\n",
       "         'part': 365,\n",
       "         'fell': 365,\n",
       "         'being': 362,\n",
       "         'offer': 361,\n",
       "         'take': 361,\n",
       "         'higher': 361,\n",
       "         'each': 360,\n",
       "         'debt': 359,\n",
       "         'japan': 357,\n",
       "         'among': 357,\n",
       "         'index': 357,\n",
       "         'including': 355,\n",
       "         'court': 350,\n",
       "         'tax': 349,\n",
       "         'world': 348,\n",
       "         'work': 348,\n",
       "         'past': 347,\n",
       "         'according': 343,\n",
       "         'report': 343,\n",
       "         'sale': 343,\n",
       "         'operations': 342,\n",
       "         'computer': 341,\n",
       "         'trade': 341,\n",
       "         'reported': 337,\n",
       "         'our': 337,\n",
       "         'then': 335,\n",
       "         'vice': 331,\n",
       "         'economic': 330,\n",
       "         'however': 329,\n",
       "         'how': 329,\n",
       "         'plans': 328,\n",
       "         'foreign': 327,\n",
       "         'high': 326,\n",
       "         'department': 325,\n",
       "         'sold': 324,\n",
       "         'yield': 322,\n",
       "         'end': 321,\n",
       "         'less': 321,\n",
       "         'several': 320,\n",
       "         'way': 320,\n",
       "         'yen': 320,\n",
       "         'banks': 319,\n",
       "         'closed': 318,\n",
       "         'lower': 318,\n",
       "         'growth': 317,\n",
       "         'insurance': 317,\n",
       "         'increase': 316,\n",
       "         'pay': 314,\n",
       "         'issues': 311,\n",
       "         'common': 308,\n",
       "         'very': 308,\n",
       "         'bid': 305,\n",
       "         'cash': 305,\n",
       "         'where': 301,\n",
       "         'day': 301,\n",
       "         'system': 300,\n",
       "         'current': 300,\n",
       "         'bill': 299,\n",
       "         'law': 299,\n",
       "         'five': 298,\n",
       "         'bush': 298,\n",
       "         'used': 298,\n",
       "         'management': 297,\n",
       "         'due': 296,\n",
       "         'early': 295,\n",
       "         'loss': 291,\n",
       "         'traders': 291,\n",
       "         'oil': 291,\n",
       "         'officer': 290,\n",
       "         'use': 290,\n",
       "         'third': 289,\n",
       "         'few': 289,\n",
       "         'good': 288,\n",
       "         \"'re\": 288,\n",
       "         'friday': 287,\n",
       "         'added': 287,\n",
       "         'public': 286,\n",
       "         'california': 286,\n",
       "         'costs': 285,\n",
       "         'office': 285,\n",
       "         'bond': 285,\n",
       "         'already': 284,\n",
       "         'futures': 283,\n",
       "         'city': 282,\n",
       "         'might': 281,\n",
       "         'assets': 281,\n",
       "         'san': 280,\n",
       "         'director': 276,\n",
       "         'spokesman': 276,\n",
       "         'oct.': 275,\n",
       "         'small': 274,\n",
       "         'late': 274,\n",
       "         'least': 274,\n",
       "         'based': 274,\n",
       "         'though': 273,\n",
       "         'september': 273,\n",
       "         'far': 272,\n",
       "         'treasury': 271,\n",
       "         'news': 270,\n",
       "         'congress': 268,\n",
       "         'agency': 267,\n",
       "         'number': 267,\n",
       "         'concern': 266,\n",
       "         'too': 266,\n",
       "         'british': 266,\n",
       "         'same': 265,\n",
       "         'going': 264,\n",
       "         'street': 264,\n",
       "         'union': 264,\n",
       "         'real': 263,\n",
       "         'him': 262,\n",
       "         'operating': 262,\n",
       "         'think': 261,\n",
       "         'research': 261,\n",
       "         'ended': 259,\n",
       "         'home': 259,\n",
       "         'until': 258,\n",
       "         'called': 258,\n",
       "         'fund': 258,\n",
       "         'today': 257,\n",
       "         'close': 257,\n",
       "         'contract': 257,\n",
       "         'case': 257,\n",
       "         'stake': 257,\n",
       "         'second': 256,\n",
       "         'wall': 256,\n",
       "         'little': 254,\n",
       "         'dollar': 253,\n",
       "         'services': 253,\n",
       "         'move': 253,\n",
       "         'former': 249,\n",
       "         'help': 249,\n",
       "         'value': 249,\n",
       "         'control': 248,\n",
       "         'analyst': 246,\n",
       "         'corporate': 246,\n",
       "         'administration': 245,\n",
       "         'maker': 244,\n",
       "         'period': 243,\n",
       "         'put': 243,\n",
       "         'point': 243,\n",
       "         'third-quarter': 243,\n",
       "         'power': 243,\n",
       "         'credit': 242,\n",
       "         'problems': 241,\n",
       "         'results': 241,\n",
       "         'agreed': 240,\n",
       "         'agreement': 240,\n",
       "         'offering': 240,\n",
       "         'want': 239,\n",
       "         'service': 239,\n",
       "         'cost': 238,\n",
       "         'economy': 238,\n",
       "         'six': 237,\n",
       "         'production': 234,\n",
       "         'four': 234,\n",
       "         'buying': 232,\n",
       "         'long': 232,\n",
       "         'firms': 232,\n",
       "         'here': 232,\n",
       "         'country': 231,\n",
       "         'life': 231,\n",
       "         'annual': 231,\n",
       "         'points': 231,\n",
       "         'recently': 231,\n",
       "         'likely': 230,\n",
       "         'without': 230,\n",
       "         'soviet': 229,\n",
       "         'committee': 229,\n",
       "         'total': 227,\n",
       "         'around': 226,\n",
       "         'although': 226,\n",
       "         'cut': 226,\n",
       "         'see': 225,\n",
       "         'whether': 225,\n",
       "         'loans': 225,\n",
       "         'volume': 224,\n",
       "         'compared': 224,\n",
       "         'policy': 224,\n",
       "         'half': 223,\n",
       "         'increased': 223,\n",
       "         'august': 221,\n",
       "         'old': 220,\n",
       "         'notes': 220,\n",
       "         'west': 219,\n",
       "         'large': 219,\n",
       "         'set': 219,\n",
       "         'yet': 218,\n",
       "         'continue': 218,\n",
       "         'john': 216,\n",
       "         'losses': 215,\n",
       "         'right': 215,\n",
       "         'further': 213,\n",
       "         'go': 213,\n",
       "         'declined': 212,\n",
       "         'strong': 212,\n",
       "         'political': 212,\n",
       "         'result': 211,\n",
       "         'must': 211,\n",
       "         'takeover': 210,\n",
       "         'selling': 210,\n",
       "         'making': 210,\n",
       "         'my': 209,\n",
       "         'monday': 209,\n",
       "         'judge': 209,\n",
       "         'francisco': 208,\n",
       "         'plant': 208,\n",
       "         'announced': 207,\n",
       "         'gain': 206,\n",
       "         'ual': 205,\n",
       "         'times': 204,\n",
       "         'largest': 204,\n",
       "         'paper': 204,\n",
       "         'earthquake': 203,\n",
       "         'expects': 203,\n",
       "         'businesses': 202,\n",
       "         'support': 202,\n",
       "         'held': 201,\n",
       "         'wo': 200,\n",
       "         'come': 199,\n",
       "         'level': 199,\n",
       "         'data': 199,\n",
       "         'systems': 198,\n",
       "         'london': 198,\n",
       "         'problem': 198,\n",
       "         'demand': 198,\n",
       "         'weeks': 197,\n",
       "         'area': 197,\n",
       "         'record': 196,\n",
       "         'official': 196,\n",
       "         'nov.': 196,\n",
       "         'composite': 195,\n",
       "         'become': 194,\n",
       "         'inc': 194,\n",
       "         'workers': 194,\n",
       "         'members': 193,\n",
       "         'fiscal': 193,\n",
       "         'industrial': 193,\n",
       "         'trust': 192,\n",
       "         'priced': 192,\n",
       "         'holding': 192,\n",
       "         'south': 192,\n",
       "         'certain': 190,\n",
       "         'ford': 190,\n",
       "         'association': 190,\n",
       "         'orders': 189,\n",
       "         'white': 188,\n",
       "         'development': 188,\n",
       "         'estimated': 188,\n",
       "         'give': 188,\n",
       "         'series': 186,\n",
       "         'drop': 186,\n",
       "         'decline': 185,\n",
       "         'currently': 185,\n",
       "         'took': 185,\n",
       "         'estate': 185,\n",
       "         'air': 185,\n",
       "         'employees': 185,\n",
       "         'executives': 184,\n",
       "         'tuesday': 183,\n",
       "         'future': 183,\n",
       "         'return': 183,\n",
       "         'senior': 182,\n",
       "         'health': 182,\n",
       "         'proposal': 181,\n",
       "         'despite': 181,\n",
       "         'others': 181,\n",
       "         'change': 181,\n",
       "         'ms.': 181,\n",
       "         'meeting': 180,\n",
       "         'commission': 180,\n",
       "         'building': 180,\n",
       "         'loan': 180,\n",
       "         'know': 180,\n",
       "         'chicago': 180,\n",
       "         'once': 179,\n",
       "         'senate': 179,\n",
       "         'nearly': 179,\n",
       "         'deal': 179,\n",
       "         'comment': 178,\n",
       "         'position': 178,\n",
       "         'rise': 178,\n",
       "         'later': 177,\n",
       "         'example': 177,\n",
       "         'acquisition': 177,\n",
       "         'show': 177,\n",
       "         'jones': 177,\n",
       "         'investor': 177,\n",
       "         'latest': 176,\n",
       "         'possible': 176,\n",
       "         'received': 176,\n",
       "         'damage': 176,\n",
       "         'dow': 176,\n",
       "         'need': 176,\n",
       "         'washington': 175,\n",
       "         'proposed': 175,\n",
       "         'robert': 175,\n",
       "         'drug': 175,\n",
       "         'often': 174,\n",
       "         'almost': 174,\n",
       "         'your': 174,\n",
       "         'filed': 174,\n",
       "         'paid': 174,\n",
       "         'junk': 174,\n",
       "         'offered': 172,\n",
       "         'charge': 172,\n",
       "         'line': 172,\n",
       "         'addition': 172,\n",
       "         'defense': 172,\n",
       "         'amount': 171,\n",
       "         'order': 171,\n",
       "         'nation': 171,\n",
       "         'product': 171,\n",
       "         'commercial': 171,\n",
       "         'east': 170,\n",
       "         'better': 170,\n",
       "         'force': 170,\n",
       "         'top': 170,\n",
       "         'purchase': 170,\n",
       "         'spending': 169,\n",
       "         'previous': 169,\n",
       "         'division': 167,\n",
       "         'dropped': 167,\n",
       "         'rights': 166,\n",
       "         'terms': 166,\n",
       "         'texas': 166,\n",
       "         'jaguar': 166,\n",
       "         'october': 165,\n",
       "         'expect': 165,\n",
       "         'within': 164,\n",
       "         'named': 163,\n",
       "         'outstanding': 163,\n",
       "         'trying': 162,\n",
       "         'changes': 161,\n",
       "         'nine': 161,\n",
       "         'began': 161,\n",
       "         'decision': 160,\n",
       "         'told': 160,\n",
       "         'keep': 159,\n",
       "         'technology': 159,\n",
       "         'industries': 159,\n",
       "         'information': 158,\n",
       "         'fed': 158,\n",
       "         'customers': 157,\n",
       "         'include': 156,\n",
       "         'america': 156,\n",
       "         'finance': 156,\n",
       "         'us': 156,\n",
       "         'enough': 156,\n",
       "         'managers': 155,\n",
       "         'came': 155,\n",
       "         'found': 155,\n",
       "         'budget': 155,\n",
       "         'banking': 154,\n",
       "         'every': 154,\n",
       "         'equipment': 154,\n",
       "         'shareholders': 154,\n",
       "         'computers': 154,\n",
       "         'provide': 153,\n",
       "         'james': 153,\n",
       "         'private': 153,\n",
       "         'auto': 153,\n",
       "         'never': 153,\n",
       "         'gains': 152,\n",
       "         'again': 152,\n",
       "         'programs': 152,\n",
       "         'got': 152,\n",
       "         'action': 152,\n",
       "         'states': 151,\n",
       "         'mortgage': 151,\n",
       "         'car': 150,\n",
       "         'ca': 149,\n",
       "         'warner': 149,\n",
       "         'lot': 149,\n",
       "         'transaction': 149,\n",
       "         'financing': 149,\n",
       "         'july': 148,\n",
       "         'following': 148,\n",
       "         'europe': 148,\n",
       "         'instead': 146,\n",
       "         'believe': 146,\n",
       "         'charges': 146,\n",
       "         'units': 146,\n",
       "         'makes': 146,\n",
       "         'tokyo': 146,\n",
       "         'best': 146,\n",
       "         'additional': 145,\n",
       "         'los': 145,\n",
       "         'run': 145,\n",
       "         'equity': 145,\n",
       "         'gas': 145,\n",
       "         'inflation': 144,\n",
       "         'local': 144,\n",
       "         'below': 144,\n",
       "         'buy-out': 144,\n",
       "         'european': 143,\n",
       "         'ltd.': 143,\n",
       "         'dollars': 143,\n",
       "         'food': 143,\n",
       "         'things': 143,\n",
       "         'united': 143,\n",
       "         'able': 143,\n",
       "         'restructuring': 142,\n",
       "         'asked': 142,\n",
       "         'great': 142,\n",
       "         'fall': 142,\n",
       "         'march': 142,\n",
       "         \"'ve\": 142,\n",
       "         'consumer': 141,\n",
       "         'mrs.': 141,\n",
       "         'low': 141,\n",
       "         'important': 141,\n",
       "         'legal': 141,\n",
       "         'sept.': 141,\n",
       "         'university': 140,\n",
       "         'corp': 140,\n",
       "         'steel': 140,\n",
       "         'above': 140,\n",
       "         'security': 140,\n",
       "         'family': 140,\n",
       "         'place': 140,\n",
       "         'away': 139,\n",
       "         'available': 139,\n",
       "         'options': 139,\n",
       "         'potential': 139,\n",
       "         'china': 139,\n",
       "         'continued': 139,\n",
       "         'pacific': 138,\n",
       "         '#': 138,\n",
       "         'bills': 138,\n",
       "         'construction': 138,\n",
       "         'head': 137,\n",
       "         'boston': 137,\n",
       "         'account': 137,\n",
       "         'lost': 137,\n",
       "         'special': 137,\n",
       "         'suit': 137,\n",
       "         'raise': 137,\n",
       "         'co': 137,\n",
       "         'holders': 136,\n",
       "         'marketing': 136,\n",
       "         'contracts': 136,\n",
       "         'led': 136,\n",
       "         'risk': 136,\n",
       "         'effect': 136,\n",
       "         'western': 136,\n",
       "         'june': 135,\n",
       "         'david': 135,\n",
       "         'taken': 134,\n",
       "         'manager': 134,\n",
       "         'whose': 134,\n",
       "         'claims': 134,\n",
       "         'advertising': 134,\n",
       "         'stores': 134,\n",
       "         'open': 134,\n",
       "         'subsidiary': 134,\n",
       "         'personal': 133,\n",
       "         'included': 133,\n",
       "         'full': 133,\n",
       "         'approval': 132,\n",
       "         'figures': 132,\n",
       "         'tv': 132,\n",
       "         'posted': 132,\n",
       "         'ibm': 131,\n",
       "         'working': 131,\n",
       "         'via': 131,\n",
       "         'reports': 130,\n",
       "         'either': 130,\n",
       "         'effort': 130,\n",
       "         'gold': 130,\n",
       "         'face': 130,\n",
       "         'noted': 129,\n",
       "         'statement': 129,\n",
       "         'lawyers': 129,\n",
       "         'meanwhile': 129,\n",
       "         'rather': 128,\n",
       "         'fact': 128,\n",
       "         'd.': 128,\n",
       "         'similar': 128,\n",
       "         'increases': 127,\n",
       "         'institute': 127,\n",
       "         'countries': 127,\n",
       "         'cars': 127,\n",
       "         'school': 126,\n",
       "         'airlines': 126,\n",
       "         'soon': 126,\n",
       "         'efforts': 125,\n",
       "         'profits': 125,\n",
       "         'network': 125,\n",
       "         'individual': 125,\n",
       "         'ad': 124,\n",
       "         'long-term': 124,\n",
       "         'left': 124,\n",
       "         'cases': 124,\n",
       "         'find': 124,\n",
       "         'directors': 124,\n",
       "         'secretary': 123,\n",
       "         'groups': 123,\n",
       "         'reserve': 123,\n",
       "         'domestic': 123,\n",
       "         'angeles': 123,\n",
       "         'probably': 123,\n",
       "         'known': 122,\n",
       "         'bought': 122,\n",
       "         'percentage': 122,\n",
       "         'germany': 121,\n",
       "         'portfolio': 121,\n",
       "         'parent': 121,\n",
       "         'telephone': 121,\n",
       "         'gained': 121,\n",
       "         'getting': 121,\n",
       "         'center': 121,\n",
       "         'slightly': 120,\n",
       "         'north': 120,\n",
       "         'something': 120,\n",
       "         'talks': 120,\n",
       "         'biggest': 120,\n",
       "         'reduce': 120,\n",
       "         'remain': 119,\n",
       "         'along': 119,\n",
       "         'looking': 119,\n",
       "         'look': 119,\n",
       "         'dealers': 119,\n",
       "         'machines': 119,\n",
       "         'failed': 118,\n",
       "         'strategy': 118,\n",
       "         'reached': 118,\n",
       "         'clients': 118,\n",
       "         'view': 118,\n",
       "         'coming': 117,\n",
       "         'magazine': 117,\n",
       "         'approved': 117,\n",
       "         'why': 117,\n",
       "         'previously': 117,\n",
       "         'party': 117,\n",
       "         'clear': 117,\n",
       "         'standard': 116,\n",
       "         'labor': 116,\n",
       "         'bay': 116,\n",
       "         'hard': 116,\n",
       "         'limited': 116,\n",
       "         'process': 116,\n",
       "         'chemical': 116,\n",
       "         'given': 116,\n",
       "         'calls': 116,\n",
       "         'helped': 116,\n",
       "         'attorney': 116,\n",
       "         'marks': 116,\n",
       "         'joint': 116,\n",
       "         'savings': 115,\n",
       "         'manufacturing': 115,\n",
       "         'communications': 115,\n",
       "         'using': 115,\n",
       "         'role': 115,\n",
       "         'black': 115,\n",
       "         'having': 115,\n",
       "         'saying': 115,\n",
       "         'canadian': 115,\n",
       "         'payments': 115,\n",
       "         'britain': 115,\n",
       "         'itself': 114,\n",
       "         'different': 114,\n",
       "         'question': 114,\n",
       "         'performance': 114,\n",
       "         'goods': 114,\n",
       "         'free': 114,\n",
       "         'act': 113,\n",
       "         'calif.': 113,\n",
       "         'levels': 113,\n",
       "         'makers': 113,\n",
       "         'merrill': 113,\n",
       "         'estimates': 113,\n",
       "         'went': 112,\n",
       "         'eastern': 112,\n",
       "         'team': 112,\n",
       "         'activity': 112,\n",
       "         'medical': 112,\n",
       "         'owns': 112,\n",
       "         'hong': 112,\n",
       "         'short': 112,\n",
       "         'german': 112,\n",
       "         'job': 111,\n",
       "         'property': 111,\n",
       "         'investments': 111,\n",
       "         'canada': 111,\n",
       "         'lynch': 111,\n",
       "         'institutions': 110,\n",
       "         'growing': 110,\n",
       "         'aid': 110,\n",
       "         'military': 110,\n",
       "         'fees': 110,\n",
       "         'year-earlier': 110,\n",
       "         'dividend': 110,\n",
       "         'buyers': 109,\n",
       "         'name': 109,\n",
       "         'lead': 109,\n",
       "         'huge': 109,\n",
       "         'plants': 109,\n",
       "         'range': 109,\n",
       "         'raised': 109,\n",
       "         'remains': 109,\n",
       "         'michael': 108,\n",
       "         'television': 108,\n",
       "         'interests': 108,\n",
       "         'me': 107,\n",
       "         'particularly': 107,\n",
       "         'completed': 107,\n",
       "         'columbia': 107,\n",
       "         'leaders': 107,\n",
       "         'vote': 107,\n",
       "         'legislation': 107,\n",
       "         'toward': 107,\n",
       "         'transportation': 106,\n",
       "         'includes': 106,\n",
       "         'seeking': 106,\n",
       "         'concerns': 106,\n",
       "         'pressure': 106,\n",
       "         'ever': 106,\n",
       "         'airline': 106,\n",
       "         'district': 106,\n",
       "         'taking': 106,\n",
       "         'especially': 106,\n",
       "         'started': 106,\n",
       "         'call': 106,\n",
       "         's&p': 106,\n",
       "         'seems': 106,\n",
       "         'william': 106,\n",
       "         'kong': 106,\n",
       "         'leading': 105,\n",
       "         'scheduled': 105,\n",
       "         'gm': 105,\n",
       "         'heavy': 105,\n",
       "         'project': 105,\n",
       "         'french': 105,\n",
       "         'central': 105,\n",
       "         'lines': 105,\n",
       "         'staff': 105,\n",
       "         'allow': 104,\n",
       "         'abortion': 104,\n",
       "         'deficit': 104,\n",
       "         'involved': 104,\n",
       "         'energy': 104,\n",
       "         \"'m\": 104,\n",
       "         'outside': 104,\n",
       "         'acquired': 104,\n",
       "         'really': 103,\n",
       "         'congressional': 103,\n",
       "         'richard': 103,\n",
       "         'always': 103,\n",
       "         'issued': 103,\n",
       "         'campaign': 102,\n",
       "         'meet': 102,\n",
       "         'hours': 102,\n",
       "         'producers': 102,\n",
       "         'independent': 102,\n",
       "         'care': 102,\n",
       "         'begin': 102,\n",
       "         'venture': 102,\n",
       "         'rep.': 102,\n",
       "         'hold': 101,\n",
       "         'currency': 101,\n",
       "         'april': 101,\n",
       "         'drexel': 101,\n",
       "         'themselves': 101,\n",
       "         'basis': 101,\n",
       "         'wants': 100,\n",
       "         'volatility': 100,\n",
       "         'needed': 100,\n",
       "         'conference': 100,\n",
       "         'means': 100,\n",
       "         'morgan': 100,\n",
       "         'impact': 100,\n",
       "         'estimate': 100,\n",
       "         'acquire': 99,\n",
       "         \"'ll\": 99,\n",
       "         'adds': 99,\n",
       "         'j.': 99,\n",
       "         'daily': 99,\n",
       "         'plc': 99,\n",
       "         'study': 99,\n",
       "         'consider': 99,\n",
       "         'bad': 98,\n",
       "         'dec.': 98,\n",
       "         'wednesday': 98,\n",
       "         'hit': 98,\n",
       "         'reduced': 98,\n",
       "         'try': 98,\n",
       "         'related': 98,\n",
       "         'turn': 98,\n",
       "         'man': 98,\n",
       "         'start': 98,\n",
       "         'base': 98,\n",
       "         'significant': 98,\n",
       "         'leader': 97,\n",
       "         'competition': 97,\n",
       "         'taxes': 97,\n",
       "         'test': 97,\n",
       "         'accounts': 97,\n",
       "         'quickly': 97,\n",
       "         'key': 97,\n",
       "         'earned': 97,\n",
       "         'doing': 97,\n",
       "         'projects': 97,\n",
       "         'reserves': 97,\n",
       "         'parts': 96,\n",
       "         'build': 96,\n",
       "         'prime': 96,\n",
       "         'partners': 96,\n",
       "         'community': 96,\n",
       "         'brokerage': 96,\n",
       "         'reason': 95,\n",
       "         'young': 95,\n",
       "         'rally': 95,\n",
       "         'considered': 95,\n",
       "         'press': 95,\n",
       "         'traded': 95,\n",
       "         'women': 95,\n",
       "         'measure': 95,\n",
       "         'beginning': 95,\n",
       "         'supply': 95,\n",
       "         'course': 95,\n",
       "         'produce': 95,\n",
       "         'journal': 95,\n",
       "         'longer': 94,\n",
       "         'convertible': 94,\n",
       "         'thought': 94,\n",
       "         'retail': 94,\n",
       "         'imports': 94,\n",
       "         'machine': 94,\n",
       "         'exports': 94,\n",
       "         'brokers': 94,\n",
       "         'holdings': 93,\n",
       "         'thing': 93,\n",
       "         'auction': 93,\n",
       "         'yields': 93,\n",
       "         'changed': 93,\n",
       "         'kind': 93,\n",
       "         'manufacturers': 93,\n",
       "         'majority': 93,\n",
       "         'thursday': 92,\n",
       "         'history': 92,\n",
       "         'poor': 92,\n",
       "         'done': 92,\n",
       "         'seven': 92,\n",
       "         'simply': 92,\n",
       "         'continuing': 92,\n",
       "         'subject': 92,\n",
       "         'motor': 92,\n",
       "         'stock-index': 92,\n",
       "         'fourth': 92,\n",
       "         'situation': 91,\n",
       "         'december': 91,\n",
       "         'protection': 91,\n",
       "         'turned': 91,\n",
       "         'worth': 91,\n",
       "         'war': 91,\n",
       "         'largely': 91,\n",
       "         'rules': 91,\n",
       "         'shareholder': 90,\n",
       "         'benefits': 90,\n",
       "         'children': 90,\n",
       "         'spokeswoman': 90,\n",
       "         'boost': 90,\n",
       "         'advanced': 90,\n",
       "         'cbs': 90,\n",
       "         'indeed': 90,\n",
       "         'areas': 90,\n",
       "         'near': 90,\n",
       "         'block': 90,\n",
       "         'generally': 90,\n",
       "         'men': 90,\n",
       "         'moody': 90,\n",
       "         'a.': 90,\n",
       "         'recession': 90,\n",
       "         'filing': 89,\n",
       "         'preferred': 89,\n",
       "         'electric': 89,\n",
       "         'pilots': 89,\n",
       "         'summer': 89,\n",
       "         'red': 89,\n",
       "         'settlement': 89,\n",
       "         'smaller': 89,\n",
       "         'eight': 89,\n",
       "         'planned': 89,\n",
       "         'returns': 89,\n",
       "         'note': 88,\n",
       "         'arbitrage': 88,\n",
       "         'hurt': 88,\n",
       "         'housing': 88,\n",
       "         'media': 88,\n",
       "         'dr.': 88,\n",
       "         'space': 88,\n",
       "         'became': 88,\n",
       "         'shearson': 88,\n",
       "         'seek': 88,\n",
       "         'form': 88,\n",
       "         'created': 87,\n",
       "         ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_tokens = Counter()\n",
    "\n",
    "for sample in ptb_train:\n",
    "    count_tokens.update(sample['sentence'].split())\n",
    "\n",
    "count_tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc640543",
   "metadata": {},
   "source": [
    "The token used to replace the numbers in this dataset: 'N'\n",
    "The token used to replace the rare tokens in this dataset: 'unk'\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "641578b1",
   "metadata": {},
   "source": [
    "## Task B: Fixed-Window Neural Language Models <a name='fixed_window_neural_lm'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88cee765",
   "metadata": {},
   "source": [
    "This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a FeedForward Network (FFN) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n",
    "\n",
    "In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n",
    "\n",
    "More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6808da1e",
   "metadata": {},
   "source": [
    "Let's first create a dataset of all consecutive tokens of length `window_size` from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f5c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "window_size = 3\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 100\n",
    "hidden_dim = 100\n",
    "\n",
    "\n",
    "class FixedWindowDataset(Dataset):\n",
    "    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    def __init__(self,\n",
    "                 train_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 test_dataset: datasets.arrow_dataset.Dataset,\n",
    "                 window_size: int,\n",
    "                 vocabulary_size: int\n",
    "                ):\n",
    "        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n",
    "        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n",
    "        \n",
    "        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n",
    "        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n",
    "        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n",
    "        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n",
    "        \n",
    "        assert vocabulary_size > len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_train_dataset)\n",
    "    \n",
    "    def get_encoded_test_samples(self):\n",
    "        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n",
    "        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n",
    "                          for word in token_list[:-1]]\n",
    "                         for token_list in all_token_lists\n",
    "                        ]\n",
    "        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"]) for \n",
    "                              token_list in all_token_lists]\n",
    "        return torch.tensor(all_token_ids), torch.tensor(all_next_token_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # here we need to transform the data to the format we expect at the model input\n",
    "        token_list = self.prepared_train_dataset[idx].split()\n",
    "        # having a fallback to <unk> token if an unseen word is encoded.\n",
    "        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n",
    "        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
    "        return torch.tensor(token_ids), torch.tensor(next_token_id)\n",
    "    \n",
    "    def decode_idx_to_word(self, token_id):\n",
    "        return [self.index_to_word[id_.item()] for id_ in token_id]\n",
    "    \n",
    "    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n",
    "        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n",
    "        # we also add a <start> token to include initial tokens in the sentences in the dataset\n",
    "        vocab += [\"<start>\"]\n",
    "        return vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n",
    "                                        window_size: int):\n",
    "        '''\n",
    "        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n",
    "        args:\n",
    "            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n",
    "            window_size: the window size for the language model\n",
    "        output:\n",
    "            prepared_dataset: a list of strings each containing 'window_size' tokens.\n",
    "        '''\n",
    "        \n",
    "        prepared_dataset = []\n",
    "        for sample in target_dataset:\n",
    "            sentence = \"<start> \"*(window_size-1) + sample['sentence']\n",
    "            sentence=sentence.split()\n",
    "            for idx in range(0,len(sentence)-window_size+1):\n",
    "                merge_tokens = \" \".join([sentence[j] for j in range(idx,idx+window_size)])\n",
    "                prepared_dataset.append(merge_tokens)        \n",
    "        return prepared_dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524f4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n",
    "\n",
    "# let's create a simple dataloader for this dataset\n",
    "train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be17ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> <start> <start> jefferies',\n",
       " '<start> <start> jefferies group',\n",
       " '<start> jefferies group inc.',\n",
       " 'jefferies group inc. said',\n",
       " 'group inc. said third-quarter',\n",
       " 'inc. said third-quarter net',\n",
       " 'said third-quarter net income',\n",
       " 'third-quarter net income fell',\n",
       " 'net income fell N',\n",
       " 'income fell N N',\n",
       " 'fell N N to',\n",
       " 'N N to $',\n",
       " 'N to $ N',\n",
       " 'to $ N million',\n",
       " '$ N million or',\n",
       " 'N million or N',\n",
       " 'million or N cents',\n",
       " 'or N cents a',\n",
       " 'N cents a share',\n",
       " 'cents a share from',\n",
       " 'a share from $',\n",
       " 'share from $ N',\n",
       " 'from $ N million',\n",
       " '$ N million or',\n",
       " 'N million or N',\n",
       " 'million or N cents',\n",
       " 'or N cents a',\n",
       " 'N cents a share',\n",
       " 'cents a share on',\n",
       " 'a share on more',\n",
       " 'share on more shares',\n",
       " 'on more shares a',\n",
       " 'more shares a year',\n",
       " 'shares a year earlier',\n",
       " 'a year earlier <stop>',\n",
       " '<start> <start> <start> keeping',\n",
       " '<start> <start> keeping the',\n",
       " '<start> keeping the mood',\n",
       " 'keeping the mood light',\n",
       " 'the mood light the',\n",
       " 'mood light the two',\n",
       " 'light the two then',\n",
       " 'the two then <unk>',\n",
       " 'two then <unk> and',\n",
       " 'then <unk> and <unk>',\n",
       " '<unk> and <unk> their',\n",
       " 'and <unk> their way',\n",
       " '<unk> their way through',\n",
       " 'their way through some',\n",
       " 'way through some <unk>',\n",
       " 'through some <unk> <unk>',\n",
       " 'some <unk> <unk> devised',\n",
       " '<unk> <unk> devised by',\n",
       " '<unk> devised by mr.',\n",
       " 'devised by mr. douglas',\n",
       " 'by mr. douglas as',\n",
       " 'mr. douglas as an',\n",
       " 'douglas as an alternative',\n",
       " 'as an alternative to',\n",
       " 'an alternative to <unk>',\n",
       " \"alternative to <unk> 's\",\n",
       " \"to <unk> 's dry\",\n",
       " \"<unk> 's dry <unk>\",\n",
       " \"'s dry <unk> techniques\",\n",
       " 'dry <unk> techniques and',\n",
       " '<unk> techniques and then',\n",
       " 'techniques and then with',\n",
       " 'and then with mr.',\n",
       " 'then with mr. <unk>',\n",
       " 'with mr. <unk> soared',\n",
       " 'mr. <unk> soared and',\n",
       " '<unk> soared and <unk>',\n",
       " 'soared and <unk> on',\n",
       " 'and <unk> on the',\n",
       " '<unk> on the <unk>',\n",
       " \"on the <unk> 's\",\n",
       " \"the <unk> 's tight\",\n",
       " \"<unk> 's tight <unk>\",\n",
       " \"'s tight <unk> <unk>\",\n",
       " 'tight <unk> <unk> <stop>',\n",
       " '<start> <start> <start> several',\n",
       " '<start> <start> several of',\n",
       " '<start> several of the',\n",
       " 'several of the new',\n",
       " 'of the new york',\n",
       " 'the new york stock',\n",
       " 'new york stock exchange',\n",
       " \"york stock exchange 's\",\n",
       " \"stock exchange 's own\",\n",
       " \"exchange 's own listed\",\n",
       " \"'s own listed companies\",\n",
       " 'own listed companies led',\n",
       " 'listed companies led by',\n",
       " 'companies led by giant',\n",
       " 'led by giant contel',\n",
       " 'by giant contel corp.',\n",
       " 'giant contel corp. are',\n",
       " 'contel corp. are joining',\n",
       " 'corp. are joining for',\n",
       " 'are joining for the',\n",
       " 'joining for the first',\n",
       " 'for the first time',\n",
       " 'the first time to',\n",
       " 'first time to complain',\n",
       " 'time to complain about',\n",
       " 'to complain about program',\n",
       " 'complain about program trading',\n",
       " 'about program trading and',\n",
       " 'program trading and the',\n",
       " 'trading and the exchange',\n",
       " \"and the exchange 's\",\n",
       " \"the exchange 's role\",\n",
       " \"exchange 's role in\",\n",
       " \"'s role in it\",\n",
       " 'role in it <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the company',\n",
       " '<start> the company installed',\n",
       " 'the company installed a',\n",
       " 'company installed a prototype',\n",
       " 'installed a prototype system',\n",
       " 'a prototype system that',\n",
       " 'prototype system that <unk>',\n",
       " 'system that <unk> dallas',\n",
       " 'that <unk> dallas with',\n",
       " '<unk> dallas with miami',\n",
       " 'dallas with miami over',\n",
       " 'with miami over digital',\n",
       " 'miami over digital phone',\n",
       " 'over digital phone lines',\n",
       " 'digital phone lines <stop>',\n",
       " '<start> <start> <start> richard',\n",
       " '<start> <start> richard j.',\n",
       " '<start> richard j. <unk>',\n",
       " 'richard j. <unk> was',\n",
       " 'j. <unk> was elected',\n",
       " '<unk> was elected to',\n",
       " 'was elected to the',\n",
       " 'elected to the board',\n",
       " 'to the board of',\n",
       " 'the board of this',\n",
       " 'board of this personnel',\n",
       " 'of this personnel consulting',\n",
       " 'this personnel consulting concern',\n",
       " 'personnel consulting concern increasing',\n",
       " 'consulting concern increasing its',\n",
       " 'concern increasing its size',\n",
       " 'increasing its size to',\n",
       " 'its size to nine',\n",
       " 'size to nine members',\n",
       " 'to nine members <stop>',\n",
       " '<start> <start> <start> buy-out',\n",
       " '<start> <start> buy-out offers',\n",
       " '<start> buy-out offers for',\n",
       " 'buy-out offers for maidenform',\n",
       " 'offers for maidenform are',\n",
       " \"for maidenform are n't\",\n",
       " \"maidenform are n't <unk>\",\n",
       " \"are n't <unk> says\",\n",
       " \"n't <unk> says executive\",\n",
       " '<unk> says executive vice',\n",
       " 'says executive vice president',\n",
       " 'executive vice president david',\n",
       " 'vice president david c.',\n",
       " 'president david c. <unk>',\n",
       " 'david c. <unk> but',\n",
       " 'c. <unk> but they',\n",
       " '<unk> but they are',\n",
       " \"but they are n't\",\n",
       " \"they are n't taken\",\n",
       " \"are n't taken very\",\n",
       " \"n't taken very seriously\",\n",
       " 'taken very seriously <stop>',\n",
       " '<start> <start> <start> yesterday',\n",
       " '<start> <start> yesterday the',\n",
       " '<start> yesterday the company',\n",
       " 'yesterday the company said',\n",
       " 'the company said it',\n",
       " 'company said it had',\n",
       " 'said it had filed',\n",
       " 'it had filed a',\n",
       " 'had filed a request',\n",
       " 'filed a request with',\n",
       " 'a request with the',\n",
       " 'request with the securities',\n",
       " 'with the securities and',\n",
       " 'the securities and exchange',\n",
       " 'securities and exchange commission',\n",
       " 'and exchange commission to',\n",
       " 'exchange commission to withdraw',\n",
       " 'commission to withdraw the',\n",
       " 'to withdraw the registration',\n",
       " 'withdraw the registration statement',\n",
       " 'the registration statement regarding',\n",
       " 'registration statement regarding the',\n",
       " 'statement regarding the proposed',\n",
       " 'regarding the proposed swap',\n",
       " 'the proposed swap <stop>',\n",
       " '<start> <start> <start> fidelity',\n",
       " \"<start> <start> fidelity 's\",\n",
       " \"<start> fidelity 's junk\",\n",
       " \"fidelity 's junk fund\",\n",
       " \"'s junk fund has\",\n",
       " 'junk fund has fallen',\n",
       " 'fund has fallen N',\n",
       " 'has fallen N N',\n",
       " 'fallen N N this',\n",
       " 'N N this year',\n",
       " 'N this year through',\n",
       " 'this year through oct.',\n",
       " 'year through oct. N',\n",
       " 'through oct. N lipper',\n",
       " 'oct. N lipper says',\n",
       " 'N lipper says the',\n",
       " 'lipper says the vanguard',\n",
       " 'says the vanguard fund',\n",
       " 'the vanguard fund rose',\n",
       " 'vanguard fund rose N',\n",
       " 'fund rose N N',\n",
       " 'rose N N and',\n",
       " 'N N and the',\n",
       " 'N and the t.',\n",
       " 'and the t. rowe',\n",
       " 'the t. rowe price',\n",
       " 't. rowe price fund',\n",
       " 'rowe price fund edged',\n",
       " 'price fund edged up',\n",
       " 'fund edged up N',\n",
       " 'edged up N N',\n",
       " 'up N N <stop>',\n",
       " '<start> <start> <start> revenue',\n",
       " '<start> <start> revenue eased',\n",
       " '<start> revenue eased N',\n",
       " 'revenue eased N N',\n",
       " 'eased N N to',\n",
       " 'N N to $',\n",
       " 'N to $ N',\n",
       " 'to $ N billion',\n",
       " '$ N billion from',\n",
       " 'N billion from $',\n",
       " 'billion from $ N',\n",
       " 'from $ N billion',\n",
       " '$ N billion <stop>',\n",
       " '<start> <start> <start> our',\n",
       " '<start> <start> our coffee',\n",
       " '<start> our coffee growers',\n",
       " 'our coffee growers face',\n",
       " 'coffee growers face reductions',\n",
       " 'growers face reductions in',\n",
       " 'face reductions in their',\n",
       " 'reductions in their income',\n",
       " 'in their income and',\n",
       " 'their income and this',\n",
       " 'income and this <unk>',\n",
       " 'and this <unk> them',\n",
       " 'this <unk> them to',\n",
       " '<unk> them to <unk>',\n",
       " 'them to <unk> <unk>',\n",
       " 'to <unk> <unk> <unk>',\n",
       " '<unk> <unk> <unk> crops',\n",
       " '<unk> <unk> crops for',\n",
       " '<unk> crops for coffee',\n",
       " 'crops for coffee <stop>',\n",
       " '<start> <start> <start> ad',\n",
       " '<start> <start> ad industry',\n",
       " '<start> ad industry executives',\n",
       " 'ad industry executives were',\n",
       " \"industry executives were n't\",\n",
       " \"executives were n't surprised\",\n",
       " \"were n't surprised by\",\n",
       " \"n't surprised by mr.\",\n",
       " 'surprised by mr. roman',\n",
       " \"by mr. roman 's\",\n",
       " \"mr. roman 's decision\",\n",
       " \"roman 's decision to\",\n",
       " \"'s decision to leave\",\n",
       " 'decision to leave ogilvy',\n",
       " 'to leave ogilvy <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the cross-border',\n",
       " '<start> the cross-border loan',\n",
       " 'the cross-border loan portfolio',\n",
       " 'cross-border loan portfolio reflected',\n",
       " 'loan portfolio reflected adjustment',\n",
       " 'portfolio reflected adjustment problems',\n",
       " 'reflected adjustment problems and',\n",
       " 'adjustment problems and <unk>',\n",
       " 'problems and <unk> payment',\n",
       " 'and <unk> payment patterns',\n",
       " '<unk> payment patterns the',\n",
       " 'payment patterns the bank',\n",
       " 'patterns the bank said',\n",
       " 'the bank said no',\n",
       " 'bank said no interest',\n",
       " 'said no interest payments',\n",
       " 'no interest payments from',\n",
       " 'interest payments from argentina',\n",
       " 'payments from argentina in',\n",
       " 'from argentina in the',\n",
       " 'argentina in the nine',\n",
       " 'in the nine months',\n",
       " 'the nine months and',\n",
       " 'nine months and none',\n",
       " 'months and none from',\n",
       " 'and none from brazil',\n",
       " 'none from brazil in',\n",
       " 'from brazil in the',\n",
       " 'brazil in the third',\n",
       " 'in the third quarter',\n",
       " 'the third quarter while',\n",
       " 'third quarter while venezuela',\n",
       " 'quarter while venezuela brought',\n",
       " 'while venezuela brought itself',\n",
       " 'venezuela brought itself substantially',\n",
       " 'brought itself substantially current',\n",
       " 'itself substantially current <stop>',\n",
       " '<start> <start> <start> N',\n",
       " '<start> <start> N billion',\n",
       " '<start> N billion yen',\n",
       " 'N billion yen of',\n",
       " 'billion yen of N',\n",
       " 'yen of N N',\n",
       " 'of N N bonds',\n",
       " 'N N bonds due',\n",
       " 'N bonds due nov.',\n",
       " 'bonds due nov. N',\n",
       " 'due nov. N N',\n",
       " 'nov. N N priced',\n",
       " 'N N priced at',\n",
       " 'N priced at N',\n",
       " 'priced at N to',\n",
       " 'at N to yield',\n",
       " 'N to yield N',\n",
       " 'to yield N N',\n",
       " 'yield N N via',\n",
       " 'N N via <unk>',\n",
       " 'N via <unk> international',\n",
       " 'via <unk> international <stop>',\n",
       " '<start> <start> <start> in',\n",
       " '<start> <start> in birmingham',\n",
       " '<start> in birmingham which',\n",
       " 'in birmingham which is',\n",
       " 'birmingham which is N',\n",
       " 'which is N N',\n",
       " 'is N N black',\n",
       " 'N N black whites',\n",
       " 'N black whites are',\n",
       " 'black whites are the',\n",
       " 'whites are the minority',\n",
       " 'are the minority <stop>',\n",
       " '<start> <start> <start> part',\n",
       " '<start> <start> part of',\n",
       " '<start> part of the',\n",
       " 'part of the problem',\n",
       " 'of the problem though',\n",
       " 'the problem though may',\n",
       " 'problem though may reflect',\n",
       " 'though may reflect a',\n",
       " 'may reflect a general',\n",
       " 'reflect a general <unk>',\n",
       " 'a general <unk> to',\n",
       " 'general <unk> to work',\n",
       " '<unk> to work with',\n",
       " 'to work with the',\n",
       " 'work with the urban',\n",
       " 'with the urban poor',\n",
       " 'the urban poor <stop>',\n",
       " '<start> <start> <start> all',\n",
       " '<start> <start> all architects',\n",
       " '<start> all architects want',\n",
       " 'all architects want to',\n",
       " 'architects want to be',\n",
       " 'want to be aware',\n",
       " 'to be aware of',\n",
       " 'be aware of the',\n",
       " 'aware of the ecological',\n",
       " 'of the ecological consequences',\n",
       " 'the ecological consequences of',\n",
       " 'ecological consequences of their',\n",
       " 'consequences of their work',\n",
       " 'of their work says',\n",
       " 'their work says john',\n",
       " 'work says john <unk>',\n",
       " 'says john <unk> whose',\n",
       " 'john <unk> whose new',\n",
       " '<unk> whose new york',\n",
       " 'whose new york firm',\n",
       " 'new york firm is',\n",
       " 'york firm is designing',\n",
       " 'firm is designing the',\n",
       " 'is designing the redevelopment',\n",
       " 'designing the redevelopment of',\n",
       " 'the redevelopment of times',\n",
       " 'redevelopment of times square',\n",
       " 'of times square but',\n",
       " 'times square but we',\n",
       " 'square but we ca',\n",
       " \"but we ca n't\",\n",
       " \"we ca n't all\",\n",
       " \"ca n't all carry\",\n",
       " \"n't all carry it\",\n",
       " 'all carry it to',\n",
       " 'carry it to that',\n",
       " 'it to that extreme',\n",
       " 'to that extreme <stop>',\n",
       " '<start> <start> <start> but',\n",
       " '<start> <start> but burger',\n",
       " '<start> but burger king',\n",
       " 'but burger king operators',\n",
       " 'burger king operators <unk>',\n",
       " 'king operators <unk> confirm',\n",
       " 'operators <unk> confirm using',\n",
       " '<unk> confirm using michael',\n",
       " \"confirm using michael 's\",\n",
       " \"using michael 's product\",\n",
       " \"michael 's product <stop>\",\n",
       " '<start> <start> <start> although',\n",
       " '<start> <start> although <unk>',\n",
       " '<start> although <unk> has',\n",
       " 'although <unk> has said',\n",
       " '<unk> has said it',\n",
       " 'has said it is',\n",
       " \"said it is n't\",\n",
       " \"it is n't interested\",\n",
       " \"is n't interested in\",\n",
       " \"n't interested in mounting\",\n",
       " 'interested in mounting a',\n",
       " 'in mounting a bid',\n",
       " 'mounting a bid for',\n",
       " 'a bid for jaguar',\n",
       " 'bid for jaguar dealers',\n",
       " 'for jaguar dealers said',\n",
       " 'jaguar dealers said its',\n",
       " 'dealers said its name',\n",
       " 'said its name further',\n",
       " 'its name further <unk>',\n",
       " 'name further <unk> the',\n",
       " 'further <unk> the growing',\n",
       " '<unk> the growing interest',\n",
       " 'the growing interest in',\n",
       " 'growing interest in the',\n",
       " 'interest in the british',\n",
       " 'in the british concern',\n",
       " 'the british concern <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the patients',\n",
       " '<start> the patients began',\n",
       " 'the patients began receiving',\n",
       " 'patients began receiving epo',\n",
       " 'began receiving epo <unk>',\n",
       " 'receiving epo <unk> about',\n",
       " 'epo <unk> about a',\n",
       " '<unk> about a month',\n",
       " 'about a month before',\n",
       " 'a month before their',\n",
       " 'month before their scheduled',\n",
       " 'before their scheduled surgery',\n",
       " 'their scheduled surgery <stop>',\n",
       " '<start> <start> <start> we',\n",
       " '<start> <start> we know',\n",
       " '<start> we know that',\n",
       " 'we know that very',\n",
       " 'know that very early',\n",
       " 'that very early exposure',\n",
       " 'very early exposure to',\n",
       " 'early exposure to <unk>',\n",
       " 'exposure to <unk> improves',\n",
       " 'to <unk> improves performance',\n",
       " '<unk> improves performance in',\n",
       " 'improves performance in the',\n",
       " 'performance in the first',\n",
       " 'in the first grade',\n",
       " 'the first grade but',\n",
       " 'first grade but afterward',\n",
       " 'grade but afterward the',\n",
       " 'but afterward the difference',\n",
       " 'afterward the difference is',\n",
       " 'the difference is quickly',\n",
       " 'difference is quickly <unk>',\n",
       " 'is quickly <unk> away',\n",
       " 'quickly <unk> away <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the other',\n",
       " '<start> the other areas',\n",
       " 'the other areas of',\n",
       " 'other areas of the',\n",
       " 'areas of the business',\n",
       " 'of the business storage',\n",
       " 'the business storage and',\n",
       " 'business storage and <unk>',\n",
       " 'storage and <unk> were',\n",
       " 'and <unk> were very',\n",
       " '<unk> were very good',\n",
       " 'were very good mr.',\n",
       " 'very good mr. johnson',\n",
       " 'good mr. johnson said',\n",
       " 'mr. johnson said <stop>',\n",
       " '<start> <start> <start> instead',\n",
       " '<start> <start> instead mr.',\n",
       " '<start> instead mr. nixon',\n",
       " 'instead mr. nixon reminded',\n",
       " 'mr. nixon reminded his',\n",
       " 'nixon reminded his host',\n",
       " 'reminded his host chinese',\n",
       " 'his host chinese president',\n",
       " 'host chinese president <unk>',\n",
       " 'chinese president <unk> <unk>',\n",
       " 'president <unk> <unk> that',\n",
       " '<unk> <unk> that americans',\n",
       " '<unk> that americans have',\n",
       " \"that americans have n't\",\n",
       " \"americans have n't <unk>\",\n",
       " \"have n't <unk> china\",\n",
       " \"n't <unk> china 's\",\n",
       " \"<unk> china 's leaders\",\n",
       " \"china 's leaders for\",\n",
       " \"'s leaders for the\",\n",
       " 'leaders for the military',\n",
       " 'for the military assault',\n",
       " 'the military assault of',\n",
       " 'military assault of june',\n",
       " 'assault of june N',\n",
       " 'of june N that',\n",
       " 'june N that killed',\n",
       " 'N that killed hundreds',\n",
       " 'that killed hundreds and',\n",
       " 'killed hundreds and perhaps',\n",
       " 'hundreds and perhaps thousands',\n",
       " 'and perhaps thousands of',\n",
       " 'perhaps thousands of demonstrators',\n",
       " 'thousands of demonstrators <stop>',\n",
       " '<start> <start> <start> these',\n",
       " '<start> <start> these include',\n",
       " '<start> these include trying',\n",
       " 'these include trying to',\n",
       " 'include trying to protect',\n",
       " 'trying to protect its',\n",
       " 'to protect its print',\n",
       " 'protect its print advertising',\n",
       " 'its print advertising by',\n",
       " 'print advertising by <unk>',\n",
       " 'advertising by <unk> the',\n",
       " 'by <unk> the first',\n",
       " '<unk> the first amendment',\n",
       " 'the first amendment and',\n",
       " 'first amendment and wooing',\n",
       " 'amendment and wooing blacks',\n",
       " 'and wooing blacks by',\n",
       " 'wooing blacks by portraying',\n",
       " 'blacks by portraying itself',\n",
       " 'by portraying itself as',\n",
       " 'portraying itself as a',\n",
       " 'itself as a <unk>',\n",
       " 'as a <unk> of',\n",
       " 'a <unk> of civil',\n",
       " '<unk> of civil rights',\n",
       " 'of civil rights <stop>',\n",
       " '<start> <start> <start> next',\n",
       " '<start> <start> next to',\n",
       " '<start> next to <unk>',\n",
       " 'next to <unk> <unk>',\n",
       " 'to <unk> <unk> a',\n",
       " '<unk> <unk> a homeless',\n",
       " '<unk> a homeless couple',\n",
       " 'a homeless couple <unk>',\n",
       " 'homeless couple <unk> into',\n",
       " 'couple <unk> into a',\n",
       " '<unk> into a blue',\n",
       " 'into a blue sleeping',\n",
       " 'a blue sleeping bag',\n",
       " 'blue sleeping bag sat',\n",
       " 'sleeping bag sat up',\n",
       " 'bag sat up said',\n",
       " 'sat up said good',\n",
       " 'up said good morning',\n",
       " 'said good morning and',\n",
       " 'good morning and then',\n",
       " 'morning and then the',\n",
       " 'and then the woman',\n",
       " 'then the woman <unk>',\n",
       " 'the woman <unk> said',\n",
       " 'woman <unk> said is',\n",
       " \"<unk> said is n't\",\n",
       " \"said is n't it\",\n",
       " \"is n't it great\",\n",
       " \"n't it great just\",\n",
       " 'it great just to',\n",
       " 'great just to be',\n",
       " 'just to be alive',\n",
       " 'to be alive <stop>',\n",
       " '<start> <start> <start> <unk>',\n",
       " '<start> <start> <unk> reformers',\n",
       " '<start> <unk> reformers can',\n",
       " '<unk> reformers can recall',\n",
       " 'reformers can recall the',\n",
       " 'can recall the <unk>',\n",
       " 'recall the <unk> <unk>',\n",
       " 'the <unk> <unk> of',\n",
       " '<unk> <unk> of the',\n",
       " '<unk> of the same',\n",
       " 'of the same period',\n",
       " 'the same period in',\n",
       " 'same period in their',\n",
       " 'period in their country',\n",
       " 'in their country <stop>',\n",
       " '<start> <start> <start> small',\n",
       " '<start> <start> small net',\n",
       " '<start> small net inflows',\n",
       " 'small net inflows into',\n",
       " 'net inflows into stock',\n",
       " 'inflows into stock and',\n",
       " 'into stock and bond',\n",
       " 'stock and bond funds',\n",
       " 'and bond funds were',\n",
       " 'bond funds were offset',\n",
       " 'funds were offset by',\n",
       " 'were offset by slight',\n",
       " 'offset by slight declines',\n",
       " 'by slight declines in',\n",
       " 'slight declines in the',\n",
       " 'declines in the value',\n",
       " 'in the value of',\n",
       " 'the value of mutual',\n",
       " 'value of mutual fund',\n",
       " 'of mutual fund stock',\n",
       " 'mutual fund stock and',\n",
       " 'fund stock and bond',\n",
       " 'stock and bond portfolios',\n",
       " 'and bond portfolios stemming',\n",
       " 'bond portfolios stemming from',\n",
       " 'portfolios stemming from falling',\n",
       " 'stemming from falling prices',\n",
       " 'from falling prices said',\n",
       " 'falling prices said jacob',\n",
       " 'prices said jacob <unk>',\n",
       " 'said jacob <unk> the',\n",
       " 'jacob <unk> the institute',\n",
       " \"<unk> the institute 's\",\n",
       " \"the institute 's chief\",\n",
       " \"institute 's chief economist\",\n",
       " \"'s chief economist <stop>\",\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the company',\n",
       " '<start> the company which',\n",
       " 'the company which reported',\n",
       " 'company which reported that',\n",
       " 'which reported that its',\n",
       " 'reported that its loss',\n",
       " 'that its loss for',\n",
       " 'its loss for the',\n",
       " 'loss for the fiscal',\n",
       " 'for the fiscal quarter',\n",
       " 'the fiscal quarter ended',\n",
       " 'fiscal quarter ended aug.',\n",
       " 'quarter ended aug. N',\n",
       " 'ended aug. N widened',\n",
       " 'aug. N widened from',\n",
       " 'N widened from a',\n",
       " 'widened from a year',\n",
       " 'from a year earlier',\n",
       " 'a year earlier cut',\n",
       " 'year earlier cut its',\n",
       " 'earlier cut its semiannual',\n",
       " 'cut its semiannual dividend',\n",
       " 'its semiannual dividend in',\n",
       " 'semiannual dividend in half',\n",
       " 'dividend in half in',\n",
       " 'in half in response',\n",
       " 'half in response to',\n",
       " 'in response to the',\n",
       " 'response to the earnings',\n",
       " 'to the earnings weakness',\n",
       " 'the earnings weakness <stop>',\n",
       " '<start> <start> <start> not',\n",
       " '<start> <start> not a',\n",
       " '<start> not a <unk>',\n",
       " 'not a <unk> question',\n",
       " 'a <unk> question unless',\n",
       " '<unk> question unless you',\n",
       " \"question unless you 're\",\n",
       " \"unless you 're the\",\n",
       " \"you 're the <unk>\",\n",
       " \"'re the <unk> <unk>\",\n",
       " 'the <unk> <unk> of',\n",
       " '<unk> <unk> of this',\n",
       " '<unk> of this city',\n",
       " \"of this city 's\",\n",
       " \"this city 's <unk>\",\n",
       " \"city 's <unk> <unk>\",\n",
       " \"'s <unk> <unk> restaurant\",\n",
       " '<unk> <unk> restaurant and',\n",
       " '<unk> restaurant and you',\n",
       " \"restaurant and you 've\",\n",
       " \"and you 've just\",\n",
       " \"you 've just lost\",\n",
       " \"'ve just lost your\",\n",
       " 'just lost your <unk>',\n",
       " 'lost your <unk> personal',\n",
       " 'your <unk> personal <unk>',\n",
       " '<unk> personal <unk> notebook',\n",
       " 'personal <unk> notebook <stop>',\n",
       " '<start> <start> <start> defense',\n",
       " '<start> <start> defense intellectuals',\n",
       " '<start> defense intellectuals have',\n",
       " 'defense intellectuals have complained',\n",
       " 'intellectuals have complained for',\n",
       " 'have complained for years',\n",
       " 'complained for years that',\n",
       " 'for years that the',\n",
       " 'years that the pentagon',\n",
       " 'that the pentagon can',\n",
       " 'the pentagon can not',\n",
       " 'pentagon can not determine',\n",
       " 'can not determine priorities',\n",
       " 'not determine priorities because',\n",
       " 'determine priorities because it',\n",
       " 'priorities because it has',\n",
       " 'because it has no',\n",
       " 'it has no strategy',\n",
       " 'has no strategy <stop>',\n",
       " '<start> <start> <start> a',\n",
       " '<start> <start> a spokeswoman',\n",
       " '<start> a spokeswoman said',\n",
       " 'a spokeswoman said toronto-based',\n",
       " 'spokeswoman said toronto-based campeau',\n",
       " 'said toronto-based campeau has',\n",
       " 'toronto-based campeau has received',\n",
       " 'campeau has received <unk>',\n",
       " 'has received <unk> of',\n",
       " 'received <unk> of interest',\n",
       " '<unk> of interest in',\n",
       " 'of interest in bloomingdale',\n",
       " \"interest in bloomingdale 's\",\n",
       " \"in bloomingdale 's but\",\n",
       " \"bloomingdale 's but she\",\n",
       " \"'s but she declined\",\n",
       " 'but she declined to',\n",
       " 'she declined to comment',\n",
       " 'declined to comment on',\n",
       " 'to comment on whether',\n",
       " 'comment on whether any',\n",
       " 'on whether any actual',\n",
       " 'whether any actual bids',\n",
       " 'any actual bids had',\n",
       " 'actual bids had been',\n",
       " 'bids had been made',\n",
       " 'had been made <stop>',\n",
       " '<start> <start> <start> racial',\n",
       " '<start> <start> racial gerrymandering',\n",
       " '<start> racial gerrymandering creating',\n",
       " 'racial gerrymandering creating separate',\n",
       " 'gerrymandering creating separate black',\n",
       " 'creating separate black and',\n",
       " 'separate black and white',\n",
       " 'black and white districts',\n",
       " 'and white districts says',\n",
       " 'white districts says that',\n",
       " 'districts says that we',\n",
       " 'says that we have',\n",
       " 'that we have discarded',\n",
       " 'we have discarded that',\n",
       " 'have discarded that belief',\n",
       " 'discarded that belief in',\n",
       " 'that belief in our',\n",
       " 'belief in our ability',\n",
       " 'in our ability to',\n",
       " 'our ability to live',\n",
       " 'ability to live together',\n",
       " 'to live together and',\n",
       " 'live together and govern',\n",
       " 'together and govern ourselves',\n",
       " 'and govern ourselves as',\n",
       " 'govern ourselves as one',\n",
       " 'ourselves as one people',\n",
       " 'as one people <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the price',\n",
       " '<start> the price of',\n",
       " 'the price of attracting',\n",
       " 'price of attracting capital',\n",
       " 'of attracting capital whether',\n",
       " 'attracting capital whether one',\n",
       " \"capital whether one 's\",\n",
       " \"whether one 's own\",\n",
       " \"one 's own or\",\n",
       " \"'s own or that\",\n",
       " 'own or that of',\n",
       " 'or that of foreigners',\n",
       " 'that of foreigners is',\n",
       " 'of foreigners is a',\n",
       " 'foreigners is a trade',\n",
       " 'is a trade deficit',\n",
       " 'a trade deficit <stop>',\n",
       " '<start> <start> <start> conservative',\n",
       " '<start> <start> conservative republicans',\n",
       " '<start> conservative republicans will',\n",
       " 'conservative republicans will be',\n",
       " 'republicans will be given',\n",
       " 'will be given the',\n",
       " 'be given the choice',\n",
       " 'given the choice of',\n",
       " 'the choice of supporting',\n",
       " 'choice of supporting or',\n",
       " 'of supporting or fighting',\n",
       " 'supporting or fighting their',\n",
       " 'or fighting their party',\n",
       " \"fighting their party 's\",\n",
       " \"their party 's popular\",\n",
       " \"party 's popular president\",\n",
       " \"'s popular president in\",\n",
       " 'popular president in an',\n",
       " 'president in an election',\n",
       " 'in an election year',\n",
       " 'an election year <stop>',\n",
       " '<start> <start> <start> diamond',\n",
       " '<start> <start> diamond creek',\n",
       " '<start> diamond creek N',\n",
       " 'diamond creek N lake',\n",
       " 'creek N lake <unk>',\n",
       " 'N lake <unk> cabernet',\n",
       " 'lake <unk> cabernet weighed',\n",
       " '<unk> cabernet weighed in',\n",
       " 'cabernet weighed in this',\n",
       " 'weighed in this fall',\n",
       " 'in this fall with',\n",
       " 'this fall with a',\n",
       " 'fall with a <unk>',\n",
       " 'with a <unk> price',\n",
       " 'a <unk> price of',\n",
       " '<unk> price of $',\n",
       " 'price of $ N',\n",
       " 'of $ N a',\n",
       " '$ N a bottle',\n",
       " 'N a bottle <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the initial',\n",
       " '<start> the initial $',\n",
       " 'the initial $ N',\n",
       " 'initial $ N million',\n",
       " '$ N million research',\n",
       " 'N million research program',\n",
       " 'million research program will',\n",
       " 'research program will conduct',\n",
       " 'program will conduct the',\n",
       " 'will conduct the most',\n",
       " 'conduct the most extensive',\n",
       " 'the most extensive testing',\n",
       " 'most extensive testing to',\n",
       " 'extensive testing to date',\n",
       " 'testing to date of',\n",
       " 'to date of <unk>',\n",
       " 'date of <unk> <unk>',\n",
       " 'of <unk> <unk> said',\n",
       " '<unk> <unk> said joe',\n",
       " '<unk> said joe <unk>',\n",
       " 'said joe <unk> head',\n",
       " 'joe <unk> head of',\n",
       " '<unk> head of fuels',\n",
       " 'head of fuels and',\n",
       " 'of fuels and lubricants',\n",
       " 'fuels and lubricants at',\n",
       " 'and lubricants at general',\n",
       " 'lubricants at general motors',\n",
       " 'at general motors corp.',\n",
       " 'general motors corp. research',\n",
       " 'motors corp. research laboratories',\n",
       " 'corp. research laboratories <stop>',\n",
       " '<start> <start> <start> xtra',\n",
       " '<start> <start> xtra a',\n",
       " '<start> xtra a transportation',\n",
       " 'xtra a transportation leasing',\n",
       " 'a transportation leasing company',\n",
       " 'transportation leasing company said',\n",
       " 'leasing company said in',\n",
       " 'company said in a',\n",
       " 'said in a statement',\n",
       " 'in a statement it',\n",
       " 'a statement it would',\n",
       " 'statement it would have',\n",
       " 'it would have no',\n",
       " 'would have no comment',\n",
       " 'have no comment on',\n",
       " 'no comment on mr.',\n",
       " 'comment on mr. gintel',\n",
       " \"on mr. gintel 's\",\n",
       " \"mr. gintel 's plans\",\n",
       " \"gintel 's plans until\",\n",
       " \"'s plans until further\",\n",
       " 'plans until further information',\n",
       " 'until further information has',\n",
       " 'further information has been',\n",
       " 'information has been disclosed',\n",
       " 'has been disclosed by',\n",
       " 'been disclosed by him',\n",
       " 'disclosed by him <stop>',\n",
       " '<start> <start> <start> opec',\n",
       " \"<start> <start> opec 's\",\n",
       " \"<start> opec 's ability\",\n",
       " \"opec 's ability to\",\n",
       " \"'s ability to produce\",\n",
       " 'ability to produce more',\n",
       " 'to produce more oil',\n",
       " 'produce more oil than',\n",
       " 'more oil than it',\n",
       " 'oil than it can',\n",
       " 'than it can sell',\n",
       " 'it can sell is',\n",
       " 'can sell is starting',\n",
       " 'sell is starting to',\n",
       " 'is starting to cast',\n",
       " 'starting to cast a',\n",
       " 'to cast a shadow',\n",
       " 'cast a shadow over',\n",
       " 'a shadow over world',\n",
       " 'shadow over world oil',\n",
       " 'over world oil markets',\n",
       " 'world oil markets <stop>',\n",
       " '<start> <start> <start> they',\n",
       " '<start> <start> they are',\n",
       " \"<start> they are n't\",\n",
       " \"they are n't accepted\",\n",
       " \"are n't accepted everywhere\",\n",
       " \"n't accepted everywhere however\",\n",
       " 'accepted everywhere however <stop>',\n",
       " '<start> <start> <start> dow',\n",
       " '<start> <start> dow jones',\n",
       " '<start> dow jones industrials',\n",
       " 'dow jones industrials N',\n",
       " 'jones industrials N up',\n",
       " 'industrials N up N',\n",
       " 'N up N transportation',\n",
       " 'up N transportation N',\n",
       " 'N transportation N up',\n",
       " 'transportation N up N',\n",
       " 'N up N utilities',\n",
       " 'up N utilities N',\n",
       " 'N utilities N up',\n",
       " 'utilities N up N',\n",
       " 'N up N <stop>',\n",
       " '<start> <start> <start> a',\n",
       " '<start> <start> a settlement',\n",
       " '<start> a settlement was',\n",
       " 'a settlement was reached',\n",
       " 'settlement was reached but',\n",
       " 'was reached but was',\n",
       " \"reached but was n't\",\n",
       " \"but was n't made\",\n",
       " \"was n't made public\",\n",
       " \"n't made public <stop>\",\n",
       " '<start> <start> <start> labor',\n",
       " '<start> <start> labor <stop>',\n",
       " '<start> <start> <start> the',\n",
       " '<start> <start> the soviet',\n",
       " '<start> the soviet <unk>',\n",
       " 'the soviet <unk> in',\n",
       " 'soviet <unk> in nicaragua',\n",
       " '<unk> in nicaragua is',\n",
       " 'in nicaragua is <unk>',\n",
       " 'nicaragua is <unk> for',\n",
       " 'is <unk> for costa',\n",
       " '<unk> for costa rica',\n",
       " 'for costa rica a',\n",
       " 'costa rica a peaceful',\n",
       " 'rica a peaceful democracy',\n",
       " 'a peaceful democracy without',\n",
       " 'peaceful democracy without an',\n",
       " 'democracy without an army',\n",
       " 'without an army <stop>',\n",
       " '<start> <start> <start> in',\n",
       " '<start> <start> in this',\n",
       " '<start> in this they',\n",
       " 'in this they are',\n",
       " 'this they are aided',\n",
       " 'they are aided by',\n",
       " 'are aided by years',\n",
       " 'aided by years of',\n",
       " 'by years of american',\n",
       " 'years of american european',\n",
       " 'of american european <unk>',\n",
       " 'american european <unk> and',\n",
       " 'european <unk> and saudi',\n",
       " '<unk> and saudi support',\n",
       " 'and saudi support for',\n",
       " 'saudi support for the',\n",
       " 'support for the most',\n",
       " 'for the most extreme',\n",
       " 'the most extreme <unk>',\n",
       " 'most extreme <unk> radical',\n",
       " 'extreme <unk> radical <unk>',\n",
       " '<unk> radical <unk> <unk>',\n",
       " 'radical <unk> <unk> with',\n",
       " '<unk> <unk> with leaders',\n",
       " '<unk> with leaders whose',\n",
       " 'with leaders whose policies',\n",
       " 'leaders whose policies are',\n",
       " 'whose policies are <unk>',\n",
       " 'policies are <unk> to',\n",
       " 'are <unk> to the',\n",
       " '<unk> to the afghan',\n",
       " 'to the afghan public',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_window_dataset.prepared_test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b44e90d",
   "metadata": {},
   "source": [
    "Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
    "\n",
    "**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the FFN layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9373555c-97cb-407d-94c4-d7016c05d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class Fixed_window_language_model(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n",
    "        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n",
    "        self.activation_func = torch.tanh # the activation function\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n",
    "        \n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "     \n",
    "    def forward(self, input_ids, labels):\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
    "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
    "        logits = self.log_softmax( self.linear2(hidden_state) )\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dae5397",
   "metadata": {},
   "source": [
    "Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93779901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                                                 window_size=window_size, vocab_size=vocabulary_size)\n",
    "\n",
    "# defining the optimizer\n",
    "optimizer = optim.SGD(model_fixed_window.parameters(),\n",
    "                      lr=0.005,\n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77626d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  5000] loss: 6.795\n",
      "[1, 10000] loss: 6.280\n",
      "[1, 15000] loss: 6.132\n",
      "[1, 20000] loss: 6.053\n",
      "[1, 25000] loss: 5.975\n",
      "[1, 30000] loss: 5.944\n",
      "[1, 35000] loss: 5.929\n",
      "[1, 40000] loss: 5.887\n",
      "[1, 45000] loss: 5.849\n",
      "[1, 50000] loss: 5.813\n",
      "[1, 55000] loss: 5.812\n",
      "[1, 60000] loss: 5.783\n",
      "[1, 65000] loss: 5.762\n",
      "[1, 70000] loss: 5.757\n",
      "[1, 75000] loss: 5.753\n",
      "[1, 80000] loss: 5.756\n",
      "[1, 85000] loss: 5.675\n",
      "[1, 90000] loss: 5.716\n",
      "[2,  5000] loss: 5.537\n",
      "[2, 10000] loss: 5.594\n",
      "[2, 15000] loss: 5.577\n",
      "[2, 20000] loss: 5.566\n",
      "[2, 25000] loss: 5.563\n",
      "[2, 30000] loss: 5.533\n",
      "[2, 35000] loss: 5.549\n",
      "[2, 40000] loss: 5.535\n",
      "[2, 45000] loss: 5.531\n",
      "[2, 50000] loss: 5.520\n",
      "[2, 55000] loss: 5.517\n",
      "[2, 60000] loss: 5.521\n",
      "[2, 65000] loss: 5.510\n",
      "[2, 70000] loss: 5.527\n",
      "[2, 75000] loss: 5.535\n",
      "[2, 80000] loss: 5.541\n",
      "[2, 85000] loss: 5.501\n",
      "[2, 90000] loss: 5.477\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a tuple of (context, target)\n",
    "        context, target = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = model_fixed_window(context, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5000 == 4999. :    # print every 5000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "path_to_save = \"/home/ezradin/NLP/nlu_ex1/models\"\n",
    "# saving the trained model\n",
    "torch.save(model_fixed_window.state_dict(), os.path.join(path_to_save,\"fixed_window_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0483dd01",
   "metadata": {},
   "source": [
    "We provide a trained model, so that you can start using it right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ba51b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n",
    "model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b806f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context and 'target' ids (target is the next word after the context)\n",
    "test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f87e283d",
   "metadata": {},
   "source": [
    "We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n",
    "#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e352e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset loss is 5.5500168800354\n"
     ]
    }
   ],
   "source": [
    "def generate_test_dataset_loss(model: torch.nn.Module,\n",
    "                               test_token_ids: torch.Tensor, \n",
    "                               test_target_ids: torch.Tensor):\n",
    "    '''\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        test_token_ids: the context ids in a single tensor.\n",
    "        test_target_ids: the target ids (next token after the context) in a single tensor.\n",
    "    output:\n",
    "        avg_test_loss: The average loss of model over test dataset.\n",
    "    '''\n",
    "    batch_size = 4\n",
    "    test_loss = []\n",
    "    # Splitting the tensor into batches along the 0th dimension\n",
    "    test_token_ids_batch = torch.split(test_token_ids, batch_size, dim=0)    \n",
    "    test_target_ids_batch = torch.split(test_target_ids, batch_size, dim=0)    \n",
    "\n",
    "    for tokens_batch,target_batch in zip(test_token_ids_batch,test_target_ids_batch):\n",
    "        output_model = model(tokens_batch,target_batch)\n",
    "        test_loss.append(output_model)\n",
    "\n",
    "    avg_test_loss =  sum(test_loss)/len(test_loss)\n",
    "    return avg_test_loss\n",
    "\n",
    "\n",
    "test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n",
    "print(f\"Test dataset loss is {test_dataset_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085d7476",
   "metadata": {},
   "source": [
    "#### Using the trained fixed-window model, implemention a function that can output entropy for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db54e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n",
    "                                            input_sequence: str,\n",
    "                                            window_size: int,\n",
    "                                            word_to_idx: dict):\n",
    "    '''\n",
    "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
    "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
    "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
    "    args:\n",
    "        model: fixed-window language model\n",
    "        input_sequence: the sequence for which we want to calculate the probability\n",
    "        window_size: the size of window for the language model\n",
    "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
    "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
    "    output:\n",
    "        sequence_entropy: the entropy for the input sequence using the trained model\n",
    "    '''\n",
    "    sequence_entropy=0\n",
    "    input_sequence = \"<start> \"*(window_size) + input_sequence\n",
    "    input_sequence = input_sequence.split()\n",
    "    for idx in range(0,len(input_sequence)-window_size):\n",
    "        #convert word to index\n",
    "        context_tokens = [word_to_idx.get(input_sequence[j], word_to_idx['<unk>']) for j in range(idx,idx+window_size)]\n",
    "        target_tokens = word_to_idx.get(input_sequence[idx+window_size],word_to_idx['<unk>'])\n",
    "        # Convert target_tokens and context_token to tensors\n",
    "        target_tokens_tensor = torch.tensor(target_tokens).unsqueeze(0)\n",
    "        context_token_tensor = torch.tensor(context_tokens).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        loss = model(context_token_tensor,target_tokens_tensor)\n",
    "        context_token_prob = math.exp(-loss.item())\n",
    "\n",
    "        sequence_entropy += -math.log2(context_token_prob)*context_token_prob\n",
    "\n",
    "    return sequence_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d055835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fixed-window model perplexity over test dataset is 1.0995\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = fixed_window_dataset.word_to_index\n",
    "\n",
    "def compute_perplexity(model, dataset, window_size, word_to_idx):\n",
    "    total_entropy = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        input_sequence = sample['sentence']\n",
    "        sequence_entropy = get_seqeuence_entropy_fixed_window_lm(model, input_sequence, window_size, word_to_idx)\n",
    "        sequence_length = len(input_sequence.split()) - window_size + 1\n",
    "\n",
    "        total_entropy += sequence_entropy\n",
    "        total_tokens += sequence_length\n",
    "\n",
    "    average_entropy = total_entropy / total_tokens\n",
    "    perplexity = 2 ** average_entropy\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Assuming you have the trained model, ptb_test dataset, window_size, and word_to_idx dictionary\n",
    "\n",
    "perplexity = compute_perplexity(model_fixed_window, ptb_test, window_size, word_to_idx)\n",
    "print(f\"The fixed-window model perplexity over test dataset is {perplexity:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240c3d89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task C: RNN-based Language Model <a name='rnn_lm'></a>\n",
    "To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n",
    "\n",
    "An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n",
    "\n",
    " As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n",
    " \n",
    "You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc631bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_language_model(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (embedding): Embedding(10000, 200)\n",
       "  (rnn): RNN(200, 200, num_layers=4)\n",
       "  (dropout): Dropout(p=0.001, inplace=False)\n",
       "  (lm_decoder): Linear(in_features=200, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnn_utils import RNNDataset, RNN_language_model\n",
    "\n",
    "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']\n",
    "\n",
    "vocabulary_size = 10000\n",
    "word_emb_dim = 200\n",
    "hidden_dim = 200\n",
    "\n",
    "rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n",
    "\n",
    "# if gpu is available, we puts the model on it \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes \n",
    "pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n",
    "rnn_model = RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
    "                               pad_idx=pad_idx)\n",
    "rnn_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25137a9f",
   "metadata": {},
   "source": [
    "load the model weights using the state_dict in `rnn_model.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52adb1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_checkpoint_file = \"rnn_model.pt\"\n",
    "rnn_model.load_state_dict(torch.load(rnn_checkpoint_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f06eb967",
   "metadata": {},
   "source": [
    "As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n",
    "As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n",
    "\n",
    "#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n",
    "**HINT**: You can use the `get_encoded_test_samples` function of `rnn_dataset` to get encoded test samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09d7b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8413/8413 [00:47<00:00, 175.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7940, device='cuda:0')\n",
      "The model perplexity is 2.4427237009778393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "test_perplexity = -1\n",
    "encode_test = rnn_dataset.get_encoded_test_samples().to(device)\n",
    "\n",
    "sequence_entropy_rnn = 0\n",
    "total_entropy_rnn = 0.0\n",
    "total_tokens_rnn = 0\n",
    "\n",
    "for i in tqdm(range(len(encode_test)-1)):\n",
    "\n",
    "    loss = rnn_model(encode_test[i:i+1])\n",
    "    context_token_prob = math.exp(-loss.item())\n",
    "    sequence_entropy_rnn += -math.log2(context_token_prob)*context_token_prob\n",
    "\n",
    "    total_entropy_rnn += sequence_entropy_rnn\n",
    "    seq_length = (encode_test[0:1] != pad_idx).sum().item()\n",
    "    total_tokens_rnn += seq_length\n",
    "\n",
    "average_entropy_rnn = total_entropy_rnn / total_tokens_rnn\n",
    "test_perplexity = 2 ** average_entropy_rnn\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    output = rnn_model(encode_test[0:2])\n",
    "print(output)\n",
    "print(f\"The model perplexity is {test_perplexity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a321bbc-352f-46a8-b05d-5174accbdc54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task D: MLM Transformer Language Models (Bonus Question: 10 pts) <a name='rnn_lm'></a>\n",
    "\n",
    "We are here interested in computing the perplexity of MLM Transformer Language Models such as BERT and RoBERTa. Hoewever, the perplexity for MLM models is not well-defined (The difference with GPT models is illstrated [here](https://huggingface.co/docs/transformers/perplexity).\n",
    "\n",
    "Instructions: First clone the following repository: https://github.com/asahi417/lmppl.\n",
    "Install the requirements and follow the instructions to compute the pseudo-perplexity [(Wang and Cho, 2019)](https://aclanthology.org/W19-2304.pdf) of 'BERT-base-uncased', 'BERT-large-uncased', 'RoBERTa-base' and 'RoBERTa-large' for the sentences:\n",
    "'Shelly ate the sliced banana with a fork' and 'The fork ate the sliced banana'.\n",
    "\n",
    "Which sentence gets the lowest pseudo-perplexity for each of the models? Which is the best model according to this test?\n",
    "What is the relation of this test to semantic roles?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30306e60-a39b-4a5d-8091-de8244bb7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55399127-3c70-4a71-9dc4-a591f9691ef3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
